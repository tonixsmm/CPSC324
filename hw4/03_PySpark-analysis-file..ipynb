{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Migrating from Spark to BigQuery via Dataproc -- Part 1\n","\n","* [Part 1](01_spark.ipynb): The original Spark code, now running on Dataproc (lift-and-shift).\n","* [Part 2](02_gcs.ipynb): Replace HDFS by Google Cloud Storage. This enables job-specific-clusters. (cloud-native)\n","* [Part 3](03_automate.ipynb): Automate everything, so that we can run in a job-specific cluster. (cloud-optimized)\n","* [Part 4](04_bigquery.ipynb): Load CSV into BigQuery, use BigQuery. (modernize)\n","* [Part 5](05_functions.ipynb): Using Cloud Functions, launch analysis every time there is a new file in the bucket. (serverless)\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting spark_analysis.py\n"]}],"source":["%%writefile spark_analysis.py\n","\n","import matplotlib\n","matplotlib.use('agg')\n","\n","import argparse\n","parser = argparse.ArgumentParser()\n","parser.add_argument(\"--bucket\", help=\"bucket for input and output\")\n","args = parser.parse_args()\n","\n","BUCKET = args.bucket"]},{"cell_type":"markdown","metadata":{},"source":["The `%%writefile spark_analysis.py` Jupyter magic command creates a new output file to contain your standalone python script. You will add a variation of this to the remaining cells to append the contents of each cell to the standalone script file.\n","\n","This code also imports the matplotlib module and explicitly sets the default plotting backend via `matplotlib.use('agg')` so that the plotting code runs outside of a Jupyter notebook."]},{"cell_type":"markdown","metadata":{},"source":["### Reading in data\n","\n","The data are CSV files. In Spark, these can be read using textFile and splitting rows on commas."]},{"cell_type":"markdown","metadata":{},"source":["The only change here is create a variable to store a Cloud Storage bucket name and then to point the data_file to the bucket we used to store the source data on Cloud Storage. Storage only requires that you repoint your storage source reference from hdfs:// to gs://."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","from pyspark.sql import SparkSession, SQLContext, Row\n","\n","gcs_bucket='qwiklabs-gcp-03-e88cc63f6e1f'\n","spark = SparkSession.builder.appName(\"kdd\").getOrCreate()\n","sc = spark.sparkContext\n","data_file = \"gs://\"+gcs_bucket+\"//kddcup.data_10_percent.gz\"\n","raw_rdd = sc.textFile(data_file).cache()\n","raw_rdd.take(5)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","csv_rdd = raw_rdd.map(lambda row: row.split(\",\"))\n","parsed_rdd = csv_rdd.map(lambda r: Row(\n","    duration=int(r[0]), \n","    protocol_type=r[1],\n","    service=r[2],\n","    flag=r[3],\n","    src_bytes=int(r[4]),\n","    dst_bytes=int(r[5]),\n","    wrong_fragment=int(r[7]),\n","    urgent=int(r[8]),\n","    hot=int(r[9]),\n","    num_failed_logins=int(r[10]),\n","    num_compromised=int(r[12]),\n","    su_attempted=r[14],\n","    num_root=int(r[15]),\n","    num_file_creations=int(r[16]),\n","    label=r[-1]\n","    )\n",")\n","parsed_rdd.take(5)"]},{"cell_type":"markdown","metadata":{},"source":["### Spark analysis\n","\n","One way to analyze data in Spark is to call methods on a dataframe."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","sqlContext = SQLContext(sc)\n","df = sqlContext.createDataFrame(parsed_rdd)\n","connections_by_protocol = df.groupBy('protocol_type').count().orderBy('count', ascending=False)\n","connections_by_protocol.show()"]},{"cell_type":"markdown","metadata":{},"source":["Another way is to use Spark SQL"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","df.registerTempTable(\"connections\")\n","attack_stats = sqlContext.sql(\"\"\"\n","                           SELECT \n","                             protocol_type, \n","                             CASE label\n","                               WHEN 'normal.' THEN 'no attack'\n","                               ELSE 'attack'\n","                             END AS state,\n","                             COUNT(*) as total_freq,\n","                             ROUND(AVG(src_bytes), 2) as mean_src_bytes,\n","                             ROUND(AVG(dst_bytes), 2) as mean_dst_bytes,\n","                             ROUND(AVG(duration), 2) as mean_duration,\n","                             SUM(num_failed_logins) as total_failed_logins,\n","                             SUM(num_compromised) as total_compromised,\n","                             SUM(num_file_creations) as total_file_creations,\n","                             SUM(su_attempted) as total_root_attempts,\n","                             SUM(num_root) as total_root_acceses\n","                           FROM connections\n","                           GROUP BY protocol_type, state\n","                           ORDER BY 3 DESC\n","                           \"\"\")\n","attack_stats.show()"]},{"cell_type":"code","execution_count":25,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","ax = attack_stats.toPandas().plot.bar(x='protocol_type', subplots=True, figsize=(10,25))"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","ax[0].get_figure().savefig('report.png');\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","import google.cloud.storage as gcs\n","bucket = gcs.Client().get_bucket(BUCKET)\n","for blob in bucket.list_blobs(prefix='sparktodp/'):\n","    blob.delete()\n","bucket.blob('sparktodp/report.png').upload_from_filename('report.png')"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Appending to spark_analysis.py\n"]}],"source":["%%writefile -a spark_analysis.py\n","\n","connections_by_protocol.write.format(\"csv\").mode(\"overwrite\").save(\n","    \"gs://{}/sparktodp/connections_by_protocol\".format(BUCKET))"]},{"cell_type":"markdown","metadata":{},"source":["### Test Automation"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing to qwiklabs-gcp-03-e88cc63f6e1f\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/04/04 01:07:19 INFO SparkEnv: Registering MapOutputTracker\n","24/04/04 01:07:19 INFO SparkEnv: Registering BlockManagerMaster\n","24/04/04 01:07:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/04/04 01:07:19 INFO SparkEnv: Registering OutputCommitCoordinator\n","/usr/lib/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n","                                                                                \r"]}],"source":["BUCKET_list = !gcloud info --format='value(config.project)'\n","BUCKET=BUCKET_list[0]\n","print('Writing to {}'.format(BUCKET))\n","!/opt/conda/miniconda3/bin/python spark_analysis.py --bucket=$BUCKET\n"]},{"cell_type":"markdown","metadata":{},"source":["This lists the script output files that have been saved to your Cloud Storage bucket."]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["gs://qwiklabs-gcp-03-e88cc63f6e1f/sparktodp/connections_by_protocol/\r\n","gs://qwiklabs-gcp-03-e88cc63f6e1f/sparktodp/connections_by_protocol/_SUCCESS\r\n","gs://qwiklabs-gcp-03-e88cc63f6e1f/sparktodp/connections_by_protocol/part-00000-bffea1c8-7017-44fd-ad2e-bf869b30eacd-c000.csv\r\n","gs://qwiklabs-gcp-03-e88cc63f6e1f/sparktodp/report.png\r\n"]}],"source":["!gcloud storage ls gs://$BUCKET/sparktodp/**"]},{"cell_type":"markdown","metadata":{},"source":["Save the Python file to a persistent storage"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://spark_analysis.py to gs://qwiklabs-gcp-03-e88cc63f6e1f/sparktodp/spark_analysis.py\n","  Completed files 1/1 | 2.8kiB/2.8kiB                                          \n"]}],"source":["!gcloud storage cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":2}